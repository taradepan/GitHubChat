{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='.env\\n__pycache__/\\ninput.wav\\noutput.mp3' metadata={'source': '.gitignore', 'file_path': '.gitignore', 'file_name': '.gitignore', 'file_type': ''}\n",
      "\n",
      "\n",
      "\n",
      "page_content='import pyaudio\\nimport wave\\nimport time\\n\\ndef record_audio(output_file):\\n    audio = pyaudio.PyAudio()\\n    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)\\n    frames = []\\n\\n    try:\\n        print(\"Recording...\")\\n        start_time = time.time()\\n\\n        while time.time() - start_time < 4:\\n            data = stream.read(1024)\\n            frames.append(data)\\n    except KeyboardInterrupt:\\n        print(\"Recording stopped.\")\\n        pass\\n\\n    stream.stop_stream()\\n    stream.close()\\n    audio.terminate()\\n\\n    waveFile = wave.open(output_file, \"wb\")\\n    waveFile.setnchannels(1)\\n    waveFile.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\\n    waveFile.setframerate(16000)\\n    waveFile.writeframes(b\\'\\'.join(frames))\\n    waveFile.close()\\n\\nif __name__ == \"__main__\":\\n    record_audio(\"output.wav\")  ' metadata={'source': 'audio.py', 'file_path': 'audio.py', 'file_name': 'audio.py', 'file_type': '.py'}\n",
      "\n",
      "\n",
      "\n",
      "page_content='from dotenv import load_dotenv\\nload_dotenv()\\nimport os\\nimport pygame\\nimport openai\\nfrom voice import voice\\nfrom test import play_audio\\nimport mutagen.mp3\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\ndef chat(userInput):\\n    completion = openai.ChatCompletion.create(\\n    model=\"gpt-3.5-turbo\",\\n    max_tokens=100,\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a personal A.I. assistant and your name is \\'Bella\\'. try to answer the user\\'s questions under 100 tokens\"},\\n        {\"role\": \"user\", \"content\": userInput},\\n    ]\\n    )\\n\\n    print(completion.choices[0].message.content)\\n\\n    voice(completion.choices[0].message.content)\\n    play_audio(\"output.mp3\")\\n\\n# chat(\"Hello, what is openai?\")\\n\\ndef get_audio_duration(audio_file):\\n    audio = mutagen.mp3.MP3(audio_file)\\n    return audio.info.length\\n\\ndef play_audio(audio_file):\\n    pygame.mixer.init()\\n    pygame.mixer.music.load(audio_file)\\n    pygame.mixer.music.play()\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n\\nif __name__ == \"__main__\":\\n    audio_file = \"output.mp3\"  \\n    play_audio(audio_file)\\n\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n' metadata={'source': 'chat.py', 'file_path': 'chat.py', 'file_name': 'chat.py', 'file_type': '.py'}\n",
      "\n",
      "\n",
      "\n",
      "page_content='from dotenv import load_dotenv\\nload_dotenv()\\nfrom chat import chat\\nimport openai\\nimport os\\nfrom audio import record_audio\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\nrecord_audio(\"input.wav\")\\n\\naudio_data = open(\"input.wav\", \"rb\")\\nresponse = openai.Audio.transcribe(\"whisper-1\", audio_data)\\n\\ntranscript = response.text\\nprint(\"Transcription:\", transcript)\\n\\nchat(transcript)' metadata={'source': 'main.py', 'file_path': 'main.py', 'file_name': 'main.py', 'file_type': '.py'}\n",
      "\n",
      "\n",
      "\n",
      "page_content='**VoiceGPT**\\n\\nThis is an open source sample project.\\nTo get started with Voice GPT:\\n1. Download this repo.\\n2. Install the Required Packages.\\n3. Create `.env` file and add your OpenAI and ElevenLabs API Key.\\n4. Run the `main.py` file.\\n\\nFeel free to use this repo. But make sure to give this repo a Star. And this repo is also open to contributions.\\n' metadata={'source': 'readme.md', 'file_path': 'readme.md', 'file_name': 'readme.md', 'file_type': '.md'}\n",
      "\n",
      "\n",
      "\n",
      "page_content='openai\\npyaudio\\nwave\\ndotenv\\npygame\\nmutagen' metadata={'source': 'requirements.txt', 'file_path': 'requirements.txt', 'file_name': 'requirements.txt', 'file_type': '.txt'}\n",
      "\n",
      "\n",
      "\n",
      "page_content='from dotenv import load_dotenv\\nimport os\\nimport requests\\nload_dotenv()\\n\\nCHUNK_SIZE = 1024\\nurl = \"https://api.elevenlabs.io/v1/text-to-speech/EXAVITQu4vr4xnSDxMaL\"\\n\\nheaders = {\\n    \"Accept\": \"audio/mpeg\",\\n    \"Content-Type\": \"application/json\",\\n    \"xi-api-key\": os.getenv(\"EL_KEY\"),\\n}\\n\\ndef voice(ai):\\n    data = {\\n        \"text\": ai,\\n        \"model_id\": \"eleven_monolingual_v1\",\\n        \"voice_settings\": {\\n            \"stability\": 0.5,\\n            \"similarity_boost\": 0.5\\n        }\\n    }\\n\\n    response = requests.post(url, json=data, headers=headers)\\n    with open(\\'output.mp3\\', \\'wb\\') as f:\\n        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\\n            if chunk:\\n                f.write(chunk)\\n\\n    \\nif __name__ == \"__main__\":\\n    text_to_speak = \"Hello, this is a test 2.\"\\n    voice(text_to_speak)\\n' metadata={'source': 'voice.py', 'file_path': 'voice.py', 'file_name': 'voice.py', 'file_type': '.py'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "# Initialize the GitLoader\n",
    "loader = GitLoader(\n",
    "    repo_path=\"./data/\",\n",
    "    clone_url=\"https://github.com/taradepan/VoiceGPT\",\n",
    "    branch=\"main\",\n",
    ")\n",
    "\n",
    "# Load the data from the GitHub repository\n",
    "data = loader.load()\n",
    "\n",
    "# Print the loaded data\n",
    "for doc in data:\n",
    "    print(doc)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pyaudio\n",
      "import wave\n",
      "import time\n",
      "\n",
      "def record_audio(output_file):\n",
      "    audio = pyaudio.PyAudio()\n",
      "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)\n",
      "    frames = []\n",
      "\n",
      "    try:\n",
      "        print(\"Recording...\")\n",
      "        start_time = time.time()\n",
      "\n",
      "        while time.time() - start_time < 4:\n",
      "            data = stream.read(1024)\n",
      "            frames.append(data)\n",
      "    except KeyboardInterrupt:\n",
      "        print(\"Recording stopped.\")\n",
      "        pass\n",
      "\n",
      "    stream.stop_stream()\n",
      "    stream.close()\n",
      "    audio.terminate()\n",
      "\n",
      "    waveFile = wave.open(output_file, \"wb\")\n",
      "    waveFile.setnchannels(1)\n",
      "    waveFile.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
      "    waveFile.setframerate(16000)\n",
      "    waveFile.writeframes(b''.join(frames))\n",
      "    waveFile.close()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    record_audio(\"output.wav\")  \n"
     ]
    }
   ],
   "source": [
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=\"HUGGING_FACE_TOKEN\", model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "db = Chroma.from_documents(data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='**VoiceGPT**\\n\\nThis is an open source sample project.\\nTo get started with Voice GPT:\\n1. Download this repo.\\n2. Install the Required Packages.\\n3. Create `.env` file and add your OpenAI and ElevenLabs API Key.\\n4. Run the `main.py` file.\\n\\nFeel free to use this repo. But make sure to give this repo a Star. And this repo is also open to contributions.\\n', metadata={'file_name': 'readme.md', 'file_path': 'readme.md', 'file_type': '.md', 'source': 'readme.md'}), Document(page_content='openai\\npyaudio\\nwave\\ndotenv\\npygame\\nmutagen', metadata={'file_name': 'requirements.txt', 'file_path': 'requirements.txt', 'file_type': '.txt', 'source': 'requirements.txt'}), Document(page_content='from dotenv import load_dotenv\\nload_dotenv()\\nimport os\\nimport pygame\\nimport openai\\nfrom voice import voice\\nfrom test import play_audio\\nimport mutagen.mp3\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\ndef chat(userInput):\\n    completion = openai.ChatCompletion.create(\\n    model=\"gpt-3.5-turbo\",\\n    max_tokens=100,\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a personal A.I. assistant and your name is \\'Bella\\'. try to answer the user\\'s questions under 100 tokens\"},\\n        {\"role\": \"user\", \"content\": userInput},\\n    ]\\n    )\\n\\n    print(completion.choices[0].message.content)\\n\\n    voice(completion.choices[0].message.content)\\n    play_audio(\"output.mp3\")\\n\\n# chat(\"Hello, what is openai?\")\\n\\ndef get_audio_duration(audio_file):\\n    audio = mutagen.mp3.MP3(audio_file)\\n    return audio.info.length\\n\\ndef play_audio(audio_file):\\n    pygame.mixer.init()\\n    pygame.mixer.music.load(audio_file)\\n    pygame.mixer.music.play()\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n\\nif __name__ == \"__main__\":\\n    audio_file = \"output.mp3\"  \\n    play_audio(audio_file)\\n\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n', metadata={'file_name': 'chat.py', 'file_path': 'chat.py', 'file_type': '.py', 'source': 'chat.py'}), Document(page_content='from dotenv import load_dotenv\\nload_dotenv()\\nfrom chat import chat\\nimport openai\\nimport os\\nfrom audio import record_audio\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\nrecord_audio(\"input.wav\")\\n\\naudio_data = open(\"input.wav\", \"rb\")\\nresponse = openai.Audio.transcribe(\"whisper-1\", audio_data)\\n\\ntranscript = response.text\\nprint(\"Transcription:\", transcript)\\n\\nchat(transcript)', metadata={'file_name': 'main.py', 'file_path': 'main.py', 'file_type': '.py', 'source': 'main.py'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is name of the project?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taradepan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/taradepan/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = \"codellama/CodeLlama-34b-Instruct-hf\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 200}, huggingfacehub_api_token=\"HUGGING_FACE_TOKEN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dict_keys(['query', 'result', 'source_documents'])\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                  retriever=db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                  return_source_documents=True\n",
    "                                  )\n",
    "\n",
    "questions = input(\"User :\")\n",
    "print(questions)\n",
    "result = qa(questions)\n",
    "#\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='openai\\npyaudio\\nwave\\ndotenv\\npygame\\nmutagen', metadata={'file_name': 'requirements.txt', 'file_path': 'requirements.txt', 'file_type': '.txt', 'source': 'requirements.txt'}), Document(page_content='from dotenv import load_dotenv\\nload_dotenv()\\nimport os\\nimport pygame\\nimport openai\\nfrom voice import voice\\nfrom test import play_audio\\nimport mutagen.mp3\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\ndef chat(userInput):\\n    completion = openai.ChatCompletion.create(\\n    model=\"gpt-3.5-turbo\",\\n    max_tokens=100,\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a personal A.I. assistant and your name is \\'Bella\\'. try to answer the user\\'s questions under 100 tokens\"},\\n        {\"role\": \"user\", \"content\": userInput},\\n    ]\\n    )\\n\\n    print(completion.choices[0].message.content)\\n\\n    voice(completion.choices[0].message.content)\\n    play_audio(\"output.mp3\")\\n\\n# chat(\"Hello, what is openai?\")\\n\\ndef get_audio_duration(audio_file):\\n    audio = mutagen.mp3.MP3(audio_file)\\n    return audio.info.length\\n\\ndef play_audio(audio_file):\\n    pygame.mixer.init()\\n    pygame.mixer.music.load(audio_file)\\n    pygame.mixer.music.play()\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n\\nif __name__ == \"__main__\":\\n    audio_file = \"output.mp3\"  \\n    play_audio(audio_file)\\n\\n    audio_duration = get_audio_duration(audio_file)\\n    delay_seconds = int(audio_duration) \\n\\n    pygame.time.delay(delay_seconds * 1000 + 1000)  \\n\\n    pygame.mixer.music.stop()\\n', metadata={'file_name': 'chat.py', 'file_path': 'chat.py', 'file_type': '.py', 'source': 'chat.py'}), Document(page_content='from dotenv import load_dotenv\\nimport os\\nimport requests\\nload_dotenv()\\n\\nCHUNK_SIZE = 1024\\nurl = \"https://api.elevenlabs.io/v1/text-to-speech/EXAVITQu4vr4xnSDxMaL\"\\n\\nheaders = {\\n    \"Accept\": \"audio/mpeg\",\\n    \"Content-Type\": \"application/json\",\\n    \"xi-api-key\": os.getenv(\"EL_KEY\"),\\n}\\n\\ndef voice(ai):\\n    data = {\\n        \"text\": ai,\\n        \"model_id\": \"eleven_monolingual_v1\",\\n        \"voice_settings\": {\\n            \"stability\": 0.5,\\n            \"similarity_boost\": 0.5\\n        }\\n    }\\n\\n    response = requests.post(url, json=data, headers=headers)\\n    with open(\\'output.mp3\\', \\'wb\\') as f:\\n        for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\\n            if chunk:\\n                f.write(chunk)\\n\\n    \\nif __name__ == \"__main__\":\\n    text_to_speak = \"Hello, this is a test 2.\"\\n    voice(text_to_speak)\\n', metadata={'file_name': 'voice.py', 'file_path': 'voice.py', 'file_type': '.py', 'source': 'voice.py'}), Document(page_content='from dotenv import load_dotenv\\nload_dotenv()\\nfrom chat import chat\\nimport openai\\nimport os\\nfrom audio import record_audio\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\nrecord_audio(\"input.wav\")\\n\\naudio_data = open(\"input.wav\", \"rb\")\\nresponse = openai.Audio.transcribe(\"whisper-1\", audio_data)\\n\\ntranscript = response.text\\nprint(\"Transcription:\", transcript)\\n\\nchat(transcript)', metadata={'file_name': 'main.py', 'file_path': 'main.py', 'file_type': '.py', 'source': 'main.py'}), Document(page_content='**VoiceGPT**\\n\\nThis is an open source sample project.\\nTo get started with Voice GPT:\\n1. Download this repo.\\n2. Install the Required Packages.\\n3. Create `.env` file and add your OpenAI and ElevenLabs API Key.\\n4. Run the `main.py` file.\\n\\nFeel free to use this repo. But make sure to give this repo a Star. And this repo is also open to contributions.\\n', metadata={'file_name': 'readme.md', 'file_path': 'readme.md', 'file_type': '.md', 'source': 'readme.md'})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'What is the name of the repo?', 'ground_truths': ['VoiceGPT']}, {'query': 'Which OpenAI model is used?', 'ground_truths': ['gpt-3.5-turbo has been used for chat completion and Wisper-1 has been used for transcribing the audio input.']}, {'query': 'list the required libries?', 'ground_truths': ['the required libraries are: \\n    openai\\n    pyaudio\\n    wave\\n    dotenv\\n    pygame\\n    mutagen']}, {'query': 'what are the gitingnore files?', 'ground_truths': [' .env\\n    __pycache__/\\n    input.wav\\n    output.mp3']}, {'query': 'How to setup the project?', 'ground_truths': [' To get started with Voice GPT:\\n    1. Download this repo.\\n    2. Install the Required Packages.\\n    3. Create `.env` file and add your OpenAI and ElevenLabs API Key.\\n    4. Run the `main.py` file.']}]\n"
     ]
    }
   ],
   "source": [
    "eval_questions = [\n",
    "    \"What is the name of the repo?\",\n",
    "    \"Which OpenAI model is used?\",\n",
    "    \"list the required libries?\",\n",
    "    \"what are the gitingnore files?\",\n",
    "    \"How to setup the project?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"VoiceGPT\",  \n",
    "    \"gpt-3.5-turbo has been used for chat completion and Wisper-1 has been used for transcribing the audio input.\",\n",
    "    \"\"\"the required libraries are: \n",
    "    openai\n",
    "    pyaudio\n",
    "    wave\n",
    "    dotenv\n",
    "    pygame\n",
    "    mutagen\"\"\",\n",
    "    \"\"\" .env\n",
    "    __pycache__/\n",
    "    input.wav\n",
    "    output.mp3\"\"\",\n",
    "    \"\"\" To get started with Voice GPT:\n",
    "    1. Download this repo.\n",
    "    2. Install the Required Packages.\n",
    "    3. Create `.env` file and add your OpenAI and ElevenLabs API Key.\n",
    "    4. Run the `main.py` file.\"\"\"\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)]\n",
    "print(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "# print(os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)\n",
    "context_recall_chain = RagasEvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VoiceGPT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = qa(examples[0])\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "eval_result = faithfulness_chain(result)\n",
    "print(eval_result[\"faithfulness_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "eval_result = context_recall_chain(result)\n",
    "print(eval_result[\"context_recall_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7221705787362578\n"
     ]
    }
   ],
   "source": [
    "eval_result = answer_rel_chain(result)\n",
    "print(eval_result['answer_relevancy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0967741935483871\n"
     ]
    }
   ],
   "source": [
    "eval_result = context_rel_chain(result)\n",
    "print(eval_result['context_relevancy_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 0.75},\n",
       " {'faithfulness_score': 1.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "predictions = qa.batch(examples)\n",
    "\n",
    "# evaluate faitfulness\n",
    "print(\"evaluating...\")\n",
    "r = faithfulness_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"evaluating...\")\n",
    "r = context_recall_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'answer_relevancy_score': 0.7221705787362578},\n",
       " {'answer_relevancy_score': 0.9204680690036255},\n",
       " {'answer_relevancy_score': 0.9170556528465212},\n",
       " {'answer_relevancy_score': 0.8026047335243285},\n",
       " {'answer_relevancy_score': 0.8098258595120184}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate answer relevancy\n",
    "print(\"evaluating...\")\n",
    "r = answer_rel_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.69s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'context_relevancy_score': 0.0967741935483871},\n",
       " {'context_relevancy_score': 0.01020408163265306},\n",
       " {'context_relevancy_score': 0.061224489795918366},\n",
       " {'context_relevancy_score': 0.08064516129032258},\n",
       " {'context_relevancy_score': 0.08108108108108109}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate context relevancy\n",
    "print(\"evaluating...\")\n",
    "r = context_rel_chain.evaluate(examples, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
